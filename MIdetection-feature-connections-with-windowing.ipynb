{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7690f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336d1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_label(dataset): \n",
    "    \n",
    "\n",
    "    df = pd.read_csv(dataset)\n",
    "    alpha = df['scp_codes'].str.split(\"'\").str[1].str[-2:]=='MI'  \n",
    "    beta = df['scp_codes'].str.split(\"'\").str[1]=='NORM'       \n",
    "    df = df[alpha|beta]\n",
    "    df['label'] = df['scp_codes'].str.split(\"'\").str[1]      \n",
    "      \n",
    "    inst_c1 = df[df['label'] == 'NORM']\n",
    "    inst_c1 = inst_c1.sample(n = 3500, random_state = 1)\n",
    "    \n",
    "    inst_c2 = df[df['label'] == 'IMI']\n",
    "    inst_c2 = inst_c2.sample(n = 2000, random_state = 1)\n",
    "    \n",
    "\n",
    "    df_new = pd.concat([inst_c1, inst_c2], ignore_index = True)\n",
    "    df_new = df_new.sample(frac = 1, random_state=42)\n",
    "    \n",
    "    return  df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125817a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_and_label(dataframe):  \n",
    "    \n",
    "    X = dataframe[['ecg_id', 'filename_hr']]   \n",
    "    X = X.to_numpy()\n",
    "    y = dataframe['label']\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_y = encoder.transform(y)\n",
    "    \n",
    "    return (X, encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d0a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5500,)\n",
      "(5500,)\n"
     ]
    }
   ],
   "source": [
    "alpha = collect_and_label(\"C:/Users/Pushpam/Downloads/ptbxl_database.csv\")  \n",
    "gamma0, gamma1 = div_and_label(alpha)\n",
    "gamma0 = gamma0[:,1]\n",
    "\n",
    "print(gamma0.shape)\n",
    "print(gamma1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ece4cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['records500/06000/06288_hr' 'records500/18000/18391_hr'\n",
      " 'records500/04000/04140_hr' ... 'records500/19000/19682_hr'\n",
      " 'records500/18000/18181_hr' 'records500/07000/07602_hr']\n"
     ]
    }
   ],
   "source": [
    "print(gamma0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e58ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773322ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117aa312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import butter, filtfilt, detrend\n",
    "\n",
    "directory = 'D:/Internship/MIDataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1'\n",
    "\n",
    "X = []\n",
    "for itr in range(gamma0.shape[0]):\n",
    "    record_name = str(gamma0[itr])\n",
    "    \n",
    "    signal, meta_val = wfdb.rdsamp(directory + '/' + record_name)\n",
    "    value = signal.T\n",
    "    ecg_signals = value\n",
    "\n",
    "    X.append(detrend(ecg_signals))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ebc71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500, 12, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b671bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5500,)\n"
     ]
    }
   ],
   "source": [
    "y_data = gamma1;\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8aeeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowing X in xnew\n",
    "# xnew = np.zeros((5*(X.shape[0]),12,1000))\n",
    "xold = X\n",
    "yold = y_data\n",
    "xnew = []\n",
    "ynew = []\n",
    "patient_ids = []\n",
    "for i in range(X.shape[0]):\n",
    "    xnew.append(X[i,:,0:1000])\n",
    "    xnew.append(X[i,:,1000:2000])\n",
    "    xnew.append(X[i,:,2000:3000])\n",
    "    xnew.append(X[i,:,3000:4000])\n",
    "    xnew.append(X[i,:,4000:5000])\n",
    "    for j in range(5):\n",
    "        ynew.append(y_data[i])\n",
    "        patient_ids.append(i)\n",
    "X = np.array(xnew)\n",
    "y_data = np.array(ynew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7fa5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import coherence\n",
    "from glob import glob\n",
    "import scipy.io as sio\n",
    "import scipy.signal as sig\n",
    "\n",
    "def hilphase(y1,y2):\n",
    "    sig1_hill=sig.hilbert(y1)\n",
    "    sig2_hill=sig.hilbert(y2)\n",
    "    pdt=(np.inner(sig1_hill,np.conj(sig2_hill))/(np.sqrt(np.inner(sig1_hill,\n",
    "               np.conj(sig1_hill))*np.inner(sig2_hill,np.conj(sig2_hill)))))\n",
    "    phase = np.angle(pdt)\n",
    "    return phase\n",
    "\n",
    "def hilphaselag(y1,y2):\n",
    "    sig1_hill=sig.hilbert(y1)\n",
    "    sig2_hill=sig.hilbert(y2)\n",
    "    pdt=sum(np.sign(np.angle(sig1_hill[:])-np.angle(sig2_hill[:])))/len(sig1_hill)\n",
    "    phase = abs(pdt)\n",
    "    return phase\n",
    "\n",
    "cross_corr_matrices_list = []\n",
    "mae_matrices_list = []\n",
    "rmse_matrices_list = []\n",
    "coherence_matrices_list = []\n",
    "# feature_1_matrices_list = []\n",
    "# feature_2_matrices_list = []\n",
    "\n",
    "for patient_data in X:\n",
    "    cross_corr_matrix = np.zeros((12, 12))\n",
    "    mae_matrix = np.zeros((12, 12))\n",
    "    rmse_matrix = np.zeros((12, 12))\n",
    "    coherence_matrix = np.zeros((12, 12))\n",
    "    feature_1_matrix = np.zeros((12, 12))\n",
    "    feature_2_matrix = np.zeros((12, 12))\n",
    "    \n",
    "    for i in range(12):\n",
    "        for j in range(i, 12):\n",
    "            lead_i = patient_data[i]\n",
    "            lead_j = patient_data[j]\n",
    "            \n",
    "            cross_corr = np.corrcoef(lead_i, lead_j)[0, 1]\n",
    "            \n",
    "            mae = np.mean(np.abs(lead_i - lead_j))\n",
    "            rmse = np.sqrt(np.mean((lead_i - lead_j) ** 2))\n",
    "            \n",
    "            f, coh = coherence(lead_i, lead_j)  \n",
    "            coherence_value = np.mean(coh)  # Store the average coherence value\n",
    "            \n",
    "#             feature_1 = hilphase(lead_i, lead_j)\n",
    "#             feature_2 = hilphaselag(lead_i, lead_j)\n",
    "            \n",
    "            cross_corr_matrix[i, j] = cross_corr\n",
    "            cross_corr_matrix[j, i] = cross_corr\n",
    "            \n",
    "            mae_matrix[i, j] = mae\n",
    "            mae_matrix[j, i] = mae\n",
    "            rmse_matrix[i, j] = rmse\n",
    "            rmse_matrix[j, i] = rmse\n",
    "            \n",
    "            coherence_matrix[i, j] = coherence_value\n",
    "            coherence_matrix[j, i] = coherence_value\n",
    "            \n",
    "#             feature_1_matrix[i, j] = feature_1\n",
    "#             feature_1_matrix[j, i] = feature_1\n",
    "            \n",
    "#             feature_2_matrix[i, j] = feature_2\n",
    "#             feature_2_matrix[j, i] = feature_2            \n",
    "    \n",
    "    # Append the matrices to the corresponding lists\n",
    "    cross_corr_matrices_list.append(cross_corr_matrix)\n",
    "    \n",
    "    mae_matrices_list.append(mae_matrix)\n",
    "    rmse_matrices_list.append(rmse_matrix)\n",
    "    \n",
    "    coherence_matrices_list.append(coherence_matrix)\n",
    "    \n",
    "#     feature_1_matrices_list.append(feature_1_matrix)\n",
    "#     feature_2_matrices_list.append(feature_2_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c91d452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_K(A_i, B_i, C_i, D_i):\n",
    "    top_row = np.concatenate((A_i, B_i), axis=1)\n",
    "    bottom_row = np.concatenate((C_i, D_i), axis=1)\n",
    "    return np.concatenate((top_row, bottom_row), axis=0)\n",
    "#     return np.concatenate((A_i, D_i), axis=1)\n",
    "\n",
    "K_matrices=[]\n",
    "for i in range(X.shape[0]):\n",
    "    K_i = create_K(cross_corr_matrices_list[i], feature_1_matrices_list[i], feature_2_matrices_list[i], coherence_matrices_list[i])\n",
    "    K_matrices.append(K_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d180d196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27500, 24, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "t = np.array(K_matrices).reshape(X.shape[0],24,24,1)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e167b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "decfe8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(t, y_data,\n",
    "#                                                     test_size=0.3,\n",
    "#                                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ac0fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ecc76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D, AvgPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12fd7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(10, (3, 3), input_shape=(24, 24, 1)))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='BinaryCrossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38c090a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbc61712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51f147c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, batch_size=16,\n",
    "#           epochs=20, verbose=1, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a43bb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bada369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.predict(X_test)\n",
    "# y_pred = np.round(predictions).astype(int).transpose()\n",
    "# print(y_pred[0,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a12aacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = np.array(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8aa8f5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 1s 4ms/step\n",
      "[[273 121]\n",
      " [ 58 648]]\n",
      "0.8372727272727273\n",
      "172/172 [==============================] - 1s 6ms/step\n",
      "[[312 120]\n",
      " [ 77 591]]\n",
      "0.8209090909090909\n",
      "172/172 [==============================] - 0s 2ms/step\n",
      "[[288 115]\n",
      " [ 77 620]]\n",
      "0.8254545454545454\n",
      "172/172 [==============================] - 0s 743us/step\n",
      "[[230 157]\n",
      " [ 43 670]]\n",
      "0.8181818181818182\n",
      "172/172 [==============================] - 1s 5ms/step\n",
      "[[272 112]\n",
      " [ 77 639]]\n",
      "0.8281818181818181\n",
      "Average Accuracy: 0.826\n"
     ]
    }
   ],
   "source": [
    "# Group-k-fold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "class SilentHistory(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "sum_accuracy = 0\n",
    "for train_idx, test_idx in gkf.split(t, y_data, groups=patient_ids):\n",
    "    X_train, X_test = t[train_idx], t[test_idx]\n",
    "    y_train, y_test = y_data[train_idx], y_data[test_idx]\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(10, (3, 3), input_shape=(24, 24, 1)))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='BinaryCrossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                    batch_size=16,\n",
    "                    epochs=20,\n",
    "                    verbose=0,  # Set verbose to 0 to suppress epoch logging\n",
    "                    validation_split=0.25,\n",
    "                    callbacks=[SilentHistory()])\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    y_pred = np.round(predictions).astype(int).transpose()\n",
    "    y_pred = y_pred[0]\n",
    "\n",
    "    y_pred_majority = []\n",
    "    for patient_idx in np.unique(patient_ids[test_idx]):\n",
    "        segment_predictions = y_pred[patient_ids[test_idx] == patient_idx]\n",
    "        majority_vote = np.bincount(segment_predictions).argmax()\n",
    "        y_pred_majority.append(majority_vote)\n",
    "\n",
    "    y_test_majority = []\n",
    "    for patient_idx in np.unique(patient_ids[test_idx]):\n",
    "        segment_predictions = y_test[patient_ids[test_idx] == patient_idx]\n",
    "        majority_vote = np.bincount(segment_predictions).argmax()\n",
    "        y_test_majority.append(majority_vote)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_majority, y_pred_majority)\n",
    "    sum_accuracy += accuracy\n",
    "    print(confusion_matrix(y_test_majority, y_pred_majority))\n",
    "    print(accuracy)\n",
    "\n",
    "average_accuracy = sum_accuracy / 5\n",
    "print(\"Average Accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48364b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
